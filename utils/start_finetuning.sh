#!/bin/bash

# Assign input parameters
CUDA_VISIBLE_DEVICES="$1"
pretrained_model_path="$2"
config_name="$3"
config_path="$4"
wandb_name="$5"
data_path="$6"
checkpoints_path="$7"
tensorboard_path="$8"
log_path="$9"
gpus="${10}"

# Logging script parameters
echo "ğŸ¯ GPUs Assigned: '$gpus'"
echo "ğŸš€ Fine-tuning started with:"
echo "ğŸ–¥ï¸  CUDA Devices: $CUDA_VISIBLE_DEVICES"
echo "ğŸ“ Pretrained Model Path: $pretrained_model_path"
echo "âš™ï¸  Config Name: $config_name"
echo "ğŸ“‚ Config Path: $config_path"
echo "ğŸ“ Data Path: $data_path"
echo "ğŸ’¾ Checkpoints Path: $checkpoints_path"
echo "ğŸ“Š TensorBoard Path: $tensorboard_path"
echo "ğŸ“œ Log Path: $log_path"

# Ensure necessary directories exist
mkdir -p "$checkpoints_path" "$tensorboard_path" "$log_path"

# Function to generate a timestamp
timestamp() {
  date +"%Y-%m-%d_%H-%M-%S"
}

# Store timestamp for logging
local_timestamp=$(timestamp)

# Print important paths
printf "\nğŸ” ** Config path is: %s" "$config_path"
printf "\nğŸ” ** Data path is: %s" "$data_path"
printf "\nğŸ’¾ ** Checkpoint will be saved at: %s" "$checkpoints_path"
printf "\nğŸ“œ ** Logs will be saved at: %s\n" "$log_path"

# Run the fine-tuning process
fairseq-hydra-train \
  task.data="$data_path" \
  common.wandb_project="$wandb_name" \
  common.log_interval=50 \
  common.log_format=tqdm \
  model.w2v_path="$pretrained_model_path" \
  distributed_training.distributed_world_size="$gpus" \
  +common.tensorboard_logdir="$tensorboard_path" \
  checkpoint.save_dir="$checkpoints_path" \
  checkpoint.restore_file="$checkpoints_path/checkpoint_last.pt" \
  --config-dir "$config_path" \
  --config-name "$config_name"

echo "âœ… Fine-tuning completed!"
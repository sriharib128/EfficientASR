# PARSpeech - ASR Training and Inference Pipeline

## ğŸ“Œ Overview
PARSPeech, an open-source model that leverages unlabeled data to enhance ASR performance for Persian, Arabic, and Urdu. We develop a scalable pipeline to collect, process, and filter unlabelled speech, resulting in a 3,000-hour multilingual corpus. Using this dataset, we pre-train a multilingual acoustic model following a continuous pretraining approach to leverage its existing knowledge. Given the complex orthography of Perso-Arabic languages, we further propose the use of sentencepiece based tokenization for vocabulary construction. Fine-tuning this pre-trained model even on limited labelled data yields performance comparable to state-of-the-art (SOTA) large models(>1B) while using only a small model(300M). This highlights the efficiency of our approach in achieving SOTA with significantly lower computational requirements

## ğŸ— Folder Structure
```
ğŸ“‚ PARSpeech
 â”œâ”€â”€ run.sh                 # Main script to run the pipeline
 â”œâ”€â”€ utils
 â”‚   â”œâ”€â”€ config
 â”‚   â”‚   â””â”€â”€ config.yaml      # Configuration file for training
 â”‚   â”œâ”€â”€ data_generation
 â”‚   â”‚   â”œâ”€â”€ data_prep.sh        # Script for data preparation
 â”‚   â”‚   â”œâ”€â”€ generate_audio_report.py
 â”‚   â”‚   â”œâ”€â”€ generate_dict_analysis.py
 â”‚   â”‚   â”œâ”€â”€ manifest.py
 â”‚   â”‚   â”œâ”€â”€ sp_dict_gen.py
 â”‚   â”‚   â”œâ”€â”€ sp_labels.py
 â”‚   â”œâ”€â”€ inference
 â”‚   â”‚   â”œâ”€â”€ components.py
 â”‚   â”‚   â”œâ”€â”€ infer.py
 â”‚   â”‚   â”œâ”€â”€ save_predicted_output.py
 â”‚   â”‚   â”œâ”€â”€ wer_wav2vec.py
 â”œâ”€â”€ start_finetuning.sh    # Script to start fine-tuning
 â”œâ”€â”€ infer.sh               # Script to run inference
```

## ğŸš€ How to Use

### 1ï¸âƒ£ Run the Script
The entire pipeline is executed using `run.sh`. You need to specify the **stage number** and **language code**:
```bash
bash run.sh <stage> <language_code>
```
Example:
```bash
bash run.sh 1 ur  # Prepare training data for Urdu
bash run.sh 2 ar  # Fine-tune model for Arabic
bash run.sh 3 fa  # Prepare test data for Farsi
bash run.sh 4 ur  # Run inference for Urdu
```

### 2ï¸âƒ£ Stages in the Pipeline
| Stage | Description |
|-------|------------|
| 1 | **Prepare Training Data**: Generates manifests, labels, and dictionaries for training |
| 2 | **Fine-tune the Model**: Starts fine-tuning using wav2vec pretrained models |
| 3 | **Prepare Testing Data**: Creates manifests and dictionaries for test data |
| 4 | **Run Inference**: Performs ASR inference and calculates WER (Word Error Rate) |

## ğŸ›  Dependencies
Refer to the [Installation Guide](Installation.md)

## ğŸ”§ Configuration
Modify `config.yaml` inside `utils/config` to change training parameters.

## ğŸ“ Script Descriptions
### **run.sh** (Main script)
- Calls appropriate stage functions
- Manages dataset and model paths

### **data_prep.sh** (Data preparation script)
- Generates dataset manifests and labels
- Splits validation data
- Creates dictionaries

### **start_finetuning.sh** (Fine-tuning script)
- Loads pretrained wav2vec model
- Logs training process
- Saves fine-tuned checkpoints

### **infer.sh** (Inference script)
- Loads fine-tuned model
- Runs inference on test dataset
- Computes WER (Word Error Rate)

## ğŸ“‚ Output Structure
```
ğŸ“‚ output
 â”œâ”€â”€ checkpoints/        # Model checkpoints
 â”œâ”€â”€ results_ar/         # Arabic ASR results
 â”œâ”€â”€ results_ur/         # Urdu ASR results
 â”œâ”€â”€ results_fa/         # Farsi ASR results
 â”œâ”€â”€ tensorboard/        # Logs for visualization
 â”œâ”€â”€ logs/               # Training and inference logs
```

## ğŸ“Š Evaluation
After inference, WER results are stored in:
```
output/results_<lang>/sentence_wise_wer.csv
```


## ğŸ“¥ Download Models
Download the models from Google Drive:

ğŸ”— [Model Checkpoints (Google Drive)](https://drive.google.com/file/d/1448YTjUV_adVcM8O0cFCb3pypj_SpXpA/view?usp=drive_link)

## ğŸ‘¥ Authors

The development of PARSpeech was led by:

1) Srihari Bandarupalli - IIITH -  ğŸ“§ Email: srihari.bandarupalli@research.iiit.ac.in 

2) Bhavana Akkiraju - IIITH - ğŸ“§ Email: bhavana.akkiraju@research.iiit.ac.in


## âš–ï¸ License

PARSpeech is licensed under the MIT License - see the LICENSE file for details. The pretrained and fine-tuned models in this repository are licensed under the same terms as the code.


